\chapter{\lclam\ Tutorial}
\label{tutorial}

\section{Introduction}
\begin{sloppypar}
  This document is intended to act as a tutorial to get you started in
  the use of \lclam.  This tutorial presupposes some prior knowledge
  of proof planning\index{proof planning}, the
  rippling\index{induction!rippling|see{rippling}}\index{method!rippling|see{rippling}}\index{rippling}
  proof method\index{method}, the
  $\lambda$-calculus\index{$\lambda$-calculus} and Prolog (though not
  $\lambda$Prolog in which {\lclam} is implemented).
\end{sloppypar}

A maths tutor was once heard to say that his task was leading the
students through a marsh along the safe path, shoving them into every
sticky pool as they went past then hauling them out again.  This
tutorial has been written with this philosophy somewhat in mind.  As a
result it frequently instructs you to do things that don't work ---
the idea being to accustom you to the sort of errors and problems you
may expect to encounter from time to time.

\section{Getting Started}
Check that the environment variables {\tt
  LCLAM\_HOME}\index{LCLAM\_HOME} and {\tt TJPATH}\index{TJPATH} have
been set properly.  {\tt LCLAM\_HOME} should point to the top level
directory of your \lclam\ distribution.  {\tt TJPATH} should point to
each of the subdirectories of {\tt \$LCLAM\_HOME/src}.
%% $
You should also check that the location of the {\tt
  tjsim}\index{tjsim} executable is on your {\tt \$PATH} and on {\tt
  \$TEYJUS\_HOME}\index{TEYJUS\_HOME}.  On the whole we advise that
you add this to your {\tt .bashrc} file to save you setting them every
time you open a new window.  If you haven't set environment variables
before you may wish to get some help with this.

We also suggest that you interact with the build procedures in an
emacs window; for interacting with the program itself, we recommend
the use of the supplied ProofGeneral interface which runs within
XEmacs.  Both the Teyjus compiler and \lclam itself generate a lot of
output which emacs will allow you to scroll through.  To get a normal
command line prompt from within emacs type M-x shell.  To start the
program you type:

\begin{verbatim}
$LCLAM_HOME/bin/lclam
\end{verbatim}
%% $
or alternatively open a new .lcm file in XEmacs to start ProofGeneral
and \lclam automatically.

\section{A First Proof}
\lclam\ comes with a number of built-in conjectures\index{built-in
  conjecture} you can try to prove.  We will now look at the proof of
one of these.

\subsubsection*{Type: {\tt query\_top\_goal X assp.}\index{query\_top\_goal}\index{assp}}

{\tt assp} stands for the associativity of plus\index{associativity of 
  plus}, i.e. the theorem
$$\forall x y z.  ((x + y) + z) = (x + (y + z))$$
Something like the following should now appear on the screen.  
\begin{verbatim}
[prompt] lclam
lclam:
query_top_goal X assp.
arithmetic
forall <lc-0-2>:nat
 forall <lc-0-3>:nat
  forall <lc-0-4>:nat
   (plus (plus (<lc-0-4>, <lc-0-3>), <lc-0-2>) =
    plus (<lc-0-4>, plus (<lc-0-3>, <lc-0-2>)))
lclam:
\end{verbatim}
This gives you the name of the theory in which the goal is found (i.e.
{\tt arithmetic}) and the goal itself is pretty printed for you.  Note
that object-level variables in Teyjus are currently pretty-printed in
the form <lc-x-y>.  

\subsubsection*{Type: {\tt pds\_plan (induction\_top normal\_ind)
assp.}\index{pds\_plan}\index{induction\_top}\index{normal\_ind}\index{assp}}

This calls the standard PDS planner with the normal induction strategy
on the {\tt assp} theorem.  A lot of output will be generated by the planner, ending with
\begin{verbatim}
backtracking over
ind_strat
ind_strat
failed
backtracking over
induction_top normal_ind
induction_top normal_ind
failed
  Failed.

lclam:
\end{verbatim}
This means \lclam\ has failed to find a plan.  Don't worry about this
at the moment but scroll back through the attempt to get an idea of
what is happening.  You will notice that it records all attempts at
applying methods:
\begin{verbatim}
Attempting... 
Method application:  induction_top normal_ind
succeeded
\end{verbatim}
and 
\begin{verbatim}
Attempting... 
allFi
failed
\end{verbatim}

After each successful method attempt \lclam\ prints out the new goal
that it is working on.  It also prints out the address\index{node
  address} in the final proof plan tree\index{plan tree} that the goal
appears at.  The first of these is:
\begin{verbatim}
Method application:  induction_top normal_ind
succeeded

GOAL:
>>> forall <lc-0-2>:nat
     forall <lc-0-3>:nat
      forall <lc-0-4>:nat
       (plus (plus (<lc-0-4>, <lc-0-3>), <lc-0-2>) =
        plus (<lc-0-4>, plus (<lc-0-3>, <lc-0-2>)))
ADDRESS: nil
\end{verbatim}
This is the first successful method application\index{method
  application}.  The {\tt nil} means it appears at the top of the
proof tree\index{proof tree}.  In fact this goal is identical to the
initial goal\index{goal}\index{goal!initial}.  Why is this?

\lclam\ maintains a distinction between {\em atomic} and {\em
  compound} methods\index{method!atomic}\index{method!compound}.  A
compound method, such as {\tt (induction\_top
  normal\_ind)}\index{induction\_top}\index{normal\_ind}, does
not actually alter the goal at all.  It expands into a set of
submethods that are to be applied in a particular order (more on this
later).  So in this case the proof tree\index{proof tree} has not been
altered at all.  All that has happened is that a compound method has
been expanded.

Scroll down a bit and you will find 
\begin{verbatim}
Attempting... 
Method application:  all_i
succeeded

GOAL:
Don't know how to display goal:
allGoal nat (Y\ seqGoal (otype_of Y nat :: nil >>> app forall (tuple (nat :: abs (y\ app forall (tuple (nat :: abs (x\ app eq (tuple (app plus (tuple (app plus (tuple (x :: y :: nil)) :: Y :: nil)) :: app plus (tuple (x :: app plus (tuple (y :: Y :: nil)) :: nil)) :: nil))) :: nil))) :: nil))))
ADDRESS: 1 :: nil

Attempting... 
Moving allGoal to meta-level
succeeded

GOAL:
<lc-0-2>:nat
>>> forall <lc-0-3>:nat
     forall <lc-0-4>:nat
      (plus (plus (<lc-0-4>, <lc-0-3>), <lc-0-2>) =
       plus (<lc-0-4>, plus (<lc-0-3>, <lc-0-2>)))
ADDRESS: 1 :: 1 :: nil
\end{verbatim}
{\tt all\_i}\index{all\_i} is an atomic method\index{atomic method}.
For the logically minded this method corresponds to the All
Introduction\index{All Introduction} or generalisation inference rule
i.e. from ${\cal A}$ deduce $\forall x_i. {\cal A}$, where {\cal A} is
any well-formed formula and $x_i$ is any variable~\cite{Hamilton78}.
Since proof planning works by backwards proof (i.e. starting with the
theorem and moving back towards the axioms by applying methods) the
effect is that the universal quantifier appears to disappear.  The
formal proof represented by executing the proof plan will move
forwards from the axioms to the theorem applying inference rules.
This is an example where the link between the method and the inference
rule is particularly clear.  In many cases a method will represent a
whole sequence of inference rules (in this case the method is supposed
to refer to an LCF-style {\em tactic} which is a program for
constructing sequences of inference rules).  At this point the goal is
quantified (the {\tt allGoal}\index{allGoal} bit with which the pretty
print can't cope -- hence the ``Don't know how to display goal''
message) this quantification is then moved to the plan level (called
meta-level above) where it is hidden from the user.  This is a
technical detail which it is usually safe to ignore.  Once
the quantification is moved the planner displays the new goal.

So the new goal is $c:nat \vdash \forall y, z. ((c + y) + z) = c + (y + z)$
where $c$ is an arbitrary constant.  \lclam\ universally quantifies
$c$ but does so at the meta-level (i.e. outside the logic).  The proof 
tree at this point looks something like figure~\ref{fig:proof_tree}.
\begin{figure}
\centerline{
        \psfig{figure=fig:assp_proof_tree.eps}}
\caption{A Proof Tree}
\label{fig:proof_tree}
\end{figure}
So the address of the new node in the tree\index{proof node address}
is $[1, 1]$.  NB.  The address of a proof node is read from right to
left, (i.e. $[3, 2]$ is the 3rd child of the 2nd child of the root node).

So what went wrong with the proof?  Well we would expect the proof to
be an inductive one.  The atomic method\index{method!atomic} that
applies induction is {\tt induction\_meth}\index{induction\_meth}.
This was the last atomic method attempted and it failed.  \lclam's
source code contains a number of rewrite rules and induction schemes
only some of which are ever wanted at a particular time.  In fact,
potentially the collection could become inconsistent if people are
experimenting with different logics etc.  Therefore you always have to 
tell \lclam\ which induction scheme\index{induction scheme}, rewrite
rules\index{rewrite rule} and wave rules\index{wave rule} you
want to use at any one time.  When developing heuristics it can often
be very helpful to restrict yourself to a very small number of such
rules and schemes
to avoid confusion before widening your view to a full theory.  This
is not the case here.  We can add all the induction schemes etc. in
the arithmetic theory\index{arithmetic theory} and use them to plan
{\tt assp}\index{assp}.  To do this type:
\begin{verbatim}
lclam:
add_theory_to_induction_scheme_list arithmetic.
Done
lclam:
add_theory_to_sym_eval_list arithmetic.
Done
lclam:
set_wave_rule_to_sym_eval.
Done
lclam:
\end{verbatim}\index{add\_theory\_to\_induction\_scheme\_list}\index{add\_theory\_to\_sym\_eval\_list}\index{set\_wave\_rule\_to\_sym\_eval}  ({\tt set\_wave\_rule\_to\_sym\_eval} sets the wave rules used by the system to be the same as those rewrite rules used for symbolic evaluation).
We also will want one other rewrite rule (namely $X=X \rewrites true$) 
this is called {\tt idty}\index{idty} and is generally useful in
nearly all proofs.  So we will add this to the symbolic
evaluation\index{symbolic evaluation}
(i.e. general rewriting\index{rewriting}) list and to the wave rule
list\index{wave rule!list}.
\begin{verbatim}
lclam:
add_to_sym_eval_list idty.
Done
lclam:
set_wave_rule_to_sym_eval.
Done
lclam:
\end{verbatim}\index{add\_to\_sym\_eval\_list}\index{add\_to\_wave\_rule\_list}

Now the command {\tt pds\_plan (induction\_top normal\_ind)
assp}\index{pds\_plan}\index{induction\_top}\index{normal\_ind}\index{assp} should
end with

\begin{verbatim}
GOAL:
>>> trueP
ADDRESS: 1 :: 1 :: 1 :: 1 :: 1 :: 2 :: 1 :: nil

Attempting... 
Method application:  trivial
succeeded

GOAL:
trueGoal!
ADDRESS: 1 :: 1 :: 1 :: 1 :: 1 :: 1 :: 2 :: 1 :: nil

Attempting... 
branch closed!

reached empty agenda

Plan found (not displayed) ...
Plan Succeeded
lclam:
\end{verbatim}
Look back through the proof and try to see how it proceeded.  Paying
particular attention to the method names that it attempts, whether
they succeed or fail and the goals they produce if they succeed.
Notice also that \lclam\ sometimes ``backtracks''\index{backtrack}
over methods.  This is prolog-style backtracking and means \lclam\ is
attempting to apply a different method at that point.  Do not worry
too much about exactly what methods do, but try to get an idea of the
sequence of goals that were proved.


\subsection{Stepping through the Proof}
As well as playing the entire proof through automatically you can step 
through the proof one method at a time.  

\subsubsection*{Type {\tt step\_by\_step on}\index{step\_by\_step}}
Restart the proof with {\tt pds\_plan (induction\_top normal\_ind)
assp}\index{pds\_plan}\index{induction\_top}\index{normal\_ind}\index{assp}.  This
will restart the proof.  Almost immediately this will stop after the
application of {\tt induction\_top normal\_ind} with the prompt {\tt
Continue Planning?}\index{Continue Planning?}.  Notice you also get
some extra output:
\begin{verbatim}
c_and_node (seqGoal (nil >>> app forall (tuple (nat :: abs (z\ app
forall (tuple (nat :: abs (y\ app forall (tuple (nat :: abs (x\ app eq
(tuple (app plus (tuple (app plus (tuple (x :: y :: nil)) :: z ::
nil)) :: app plus (tuple (x :: app plus (tuple (y :: z :: nil)) ::
nil)) :: nil))) :: nil))) :: nil))) :: nil)))) nil nomethodyet nil 
\end{verbatim}
This is the current state of the plan expressed in an abbreviated form 
of \lclam's internal format.  {\tt c\_and\_node}\index{c\_and\_node}
is a proof plan node and you see the goal\index{goal} at that node {\tt seqGoal
...}\index{seqGoal}, the address of the node\index{node!address} ({\tt 
nil}), the method\index{method} ({\tt nomethodyet})\index{nomethodyet} 
and the children of the node {\tt nil}. 

\subsubsection{Type {\tt continue.}\index{continue}}

Now you get to the point where the {\tt taut}\index{taut} compound
method\index{method!compound} was
applied.  This is the tautology checking\index{tautology checking}
method that calls {\tt all\_i} above.
If you look at the trace from your last attempt you 
will see that ultimately \lclam\ backtracks out of {\tt taut}.

\subsubsection{Type {\tt backtrack.}\index{backtrack}}
This forces the planner to try a backtrack now rather than attempt all
the submethods\index{submethod} of tautology.

Now the planner will try the {\tt sym\_eval}\index{sym\_eval} compound 
method.  Again the previous plan tells you that it will ultimately
attempt {\tt ind\_strat normal\_ind}\index{ind\_strat}\index{normal\_ind} as the first successful
method.  Instead of backtracking until this is attempted try:

\subsubsection{Type {\tt try (ind\_strat normal\_ind)}\index{try}}

This immediately attempts the {\tt ind\_strat normal\_ind} method.
Continue and the {\tt
induction\_meth}\index{induction\_meth} method will be applied.  This
will produce a tree containing a child node of the root node (you
can also see the root node at the top -- notice that the method slot
has now been instantiated).  If you look carefully at the child node
you will see it contains the {\tt allGoal}\index{allGoal} constructor for
a quantified goal and the symbol {\tt **}\index{**} which means there
are two subgoals -- one for the step case\index{step case} and one for 
the base case\index{base case}.  As its next step the planner will
split these this conjoint subgoal into two separate subgoals.

\subsubsection{Type {\tt continue.} again}
This time the split child nodes are displayed.

Normally the plan would now proceed with planning the step
case\index{step case}.  But you can switch to the base 
case\index{base case} if that is 
what you are more interested in.  You can (just about!) determine the
number of the node you wish to switch to -- it appears just before the 
{\tt nomethodyet}\index{nomethodyet} place holder.  So the base case
is node {\tt (2::1::nil)}

\subsubsection{Type {\tt plan\_node (2::1::nil).}\index{plan\_node}}
You can carry on experimenting with backtracking, attempting
specific methods and directing which goal to concentrate upon.  When
you are fed up you can type {\tt abandon.}\index{abandon} to give up
the attempt.  In an idiosyncratic fashion \lclam\ will report success
at this point {\em something should really be done about this!!}.

\section{Entering and Planning Your Own Theorem}
It is likely that once you are using \lclam\ that you will want to
enter your own conjectures for proof.  You can do this by entering new 
information (typed in the internal syntax) at the command
line\index{command line} and the commands for this are listed in
chapter~\ref{basic}.

However in general you should create a personal theory file for your
work.  Exit \lclam\ by typing {\tt halt.} and set up a directory for
your own \lclam\ work.  Add this directory to your {\tt
TJPATH}\index{TJPATH}.


\subsection{A Theorem: RevQrev\index{RevQrev}}
The object of this section is to step you through the planning of the
``RevQrev'' theorem.  
RevQrev compares two different functions for reversing a list and
shows that they do the same thing.

$rev$\index{$rev$} works according to the definition:
\begin{eqnarray}
rev(nil) & = &  nil \label{def:rev1} \\
rev(X::Y) & = & rev(Y) <> X::nil  \label{def:rev2}
\end{eqnarray}
where $<>$\index{$<>$} appends\index{append} two lists together and $::$\index{$::$} conses\index{cons} an
element on to  
a list (i.e. places the element first in a new list followed by the
elements of the old list in order).  Intuitively
$rev$'s recursive 
step strips the 
first element off the top of the list and then glues it onto the end
of the reversed list using append.   

$qrev$\index{$qrev$} works according to the definition:
\begin{eqnarray}
qrev(nil, L) & = & L \label{def:qrev1}\\
qrev(H::T, L) & = & qrev(T, H::L) \label{def:qrev2}
\end{eqnarray}
$qrev$ maintains two lists and gradually strips the first element off
one and conses it to the other.  

The object here is to show that 
$$\forall l_1, l_2. rev(l_1) <> l_2 =
qrev(l_1, l_2)$$ 
To prove this goal $\lambda${\clam} has to know the equations
for $rev$ and $qrev$ and have the goal in its list of
queries.  To do this you will need to write a theory file\index{theory 
file} for \lclam.  

\subsection{The Anatomy of a Theory File\index{theory file}}
To plan a conjecture in \lclam\ you need to create a theory file.
There are many elements that can make up a theory file\index{theory
  file} including arbitrary \lprolog\ predicates.  Examples of theory
files can be found in the {\tt theories/} subdirectory of the \lclam\
distribution.  In particular this section uses the arithmetic
theory\index{arithmetic theory} as an example which can be found in
the files {\tt arithmetic.mod} and {\tt arithmetic.sig}.  There is more detail on creating theories in chapter~\ref{mktheory}.

\subsubsection{\lclam\ Syntax\index{syntax}}
\label{sec:tut:syntax}
In writing a theory file\index{theory file} you are going to need to
use \lclam's internal syntax.  This is based on Higher Order Abstract
Syntax\index{HOAS} or $\lambda$-tree 
syntax\index{$\lambda$-tree syntax}.  
A term can be a constant, a variable, an function
application\index{function application}, a $\lambda$-abstraction\index{$\lambda$-abstraction} or a tuple of terms.
\begin{equation}
t ::= {\tt A} \mid {\tt a} \mid {\tt (app \mbox{ }} t \: t {\tt )}
\mid {\tt (abs \mbox{ }
x\backslash \mbox{ }} t {\tt )} \mid {\tt tuple [} t, \ldots, t {\tt ]}
\end{equation}
where {\tt A} is a \lprolog\ variable and {\tt a} is a \lprolog\
constant.  In this syntax $s(X)$ for some variable $X$ becomes {\tt
(app s X)}\index{app}\index{abs}\index{tuple}.  $X + Y$ becomes {\tt
(app plus (tuple [X, Y]))}.  $\lambda x. s(x)$ becomes 
\verb+(abs x\ (app s x))+.

The \lprolog\ types for the contructors are:
\begin{verbatim}
type abs        (osyn -> osyn) -> osyn.
type app        osyn -> osyn -> osyn.
type tuple      (list osyn) -> osyn.

type arrow      osyn -> osyn -> osyn.
type tuple_type (list osyn) -> osyn.

type otype_of   osyn -> osyn -> osyn.
\end{verbatim}
Basic
terms (objects of type {\tt osyn}\index{osyn}) can be Prolog variables 
or constants (e.g. {\tt P} or {\tt plus}).  The constructors {\tt
  abs}\index{abs}, {\tt app}\index{app} and {\tt tuple}\index{tuple}
are then used to build up more complex terms.  {\tt app F X}
applies\index{function application}
the function {\tt F} to the argument {X}.  If there is more than one
argument then {\tt tuple} is used, so, for instance 
{\tt app plus (tuple [1, 2])} represent the term, $1 + 2$.  {\tt abs}
is used to represent
$\lambda$-abstraction\index{$\lambda$-abstraction}.  This uses
\lprolog's internal $\lambda$-abstraction mechanism.  So the function
$\lambda x. x$ would be {\verb+ abs X\ X+} in \lclam's internal
syntax.

{\tt otype\_of}\index{otype\_of} is used to assign types to terms and
{\tt arrow}\index{arrow} and {\tt tuple\_type}\index{tuple\_type} are
used to construct function types.


Mostly the format for terms should be obvious.  By convention
quantifiers\index{quantifier} take a tuple of arguments the first
being the type of the 
quantified variable and the second being a $\lambda$-abstraction\index{$\lambda$-abstraction}.  So
the formula $\forall x:nat.  x = x$ becomes \linebreak
\verb+(app forall (tuple [nat, (abs x\ (app eq (tuple [x, x])))]))+

\subsubsection{Goals\index{goal}}
A very simple theory file\index{theory file} might contain only a top
goal\index{top goal}.
This starts with the keyword {\tt top\_goal}\index{top\_goal} followed
by the name of the theory, a list of hypotheses
and then the statement of the theorem.  So a file
conjecturing only the associativity of plus\index{associativity of plus},
{\tt assp}\index{assp}, would contain

\begin{verbatim}
top_goal mytheory assp []
    (app forall (tuple [nat, 
     (abs z\ (app forall (tuple [nat, 
      (abs y\ (app forall (tuple [nat, 
       (abs x\ (app eq (tuple [
        (app plus (tuple [(app plus (tuple [x, y])),  z])), 
        (app plus (tuple [x, (app plus (tuple [y, z]))]))])))])))])))])).
\end{verbatim}

Take a good look at the syntax for the goal and make sure you
understand how it relates to the theorem.  

\subsubsection{Definitions and Lemmas\index{definition}\index{lemma}}

Another major component of a theory file is a list of
definitions\index{definition} and lemmas\index{lemma} which you 
think will be needed by the plan\index{plan}.  Mostly these are used
to ``rewrite'' sub-expressions of some goal and are often referred to
as rewrite rules\index{rewrite rule} as a result.
You should give the type of any new
function that you introduce as well as the equations that govern its
behaviour.  The type is signalled by the {\tt
has\_otype}\index{has\_otype} keyword ({\em at the 
moment type information is only used by the
generalisation\index{generalisation} method -- \lclam\ does not
enforce type correctness in the syntax}).  {\tt has\_otype} takes
three arguments: the name of the theory, the name of the new constant
and the constant's type.

So for instance, the arithmetic theory file\index{arithmetic theory}
contains the following 
type declarations:
\begin{verbatim}
has_otype arithmetic zero nat.
has_otype arithmetic s (nat arrow nat).
has_otype arithmetic plus ((tuple_type [nat, nat]) arrow nat).
has_otype arithmetic otimes ((tuple_type [nat, nat]) arrow nat).
\end{verbatim}
These can be viewed as the assertions that $0:nat$ ($0$ is of type
natural number), $s:nat \rightarrow nat$ ($s$ is a function from
natural numbers to natural numbers), $+:(nat, nat) \rightarrow nat$
(plus is a function from a pair of natural numbers to natural numbers) 
etc.
Notice that {\tt arrow}\index{arrow} is used for function types
(i.e. applications and $\lambda$-abstractions) and there is a {\tt
tuple\_type}\index{tuple\_type} for tuples.

Definitions are started by the {\tt definition}\index{definition}
keyword.  Definitions are equations that define how some function or
constant work.  So the definition of $+$ in this context is:
\begin{eqnarray}
0 + Y & = & Y \label{def:plus1} \\
s(X) + Y & = & s(X + Y)
\end{eqnarray}
A definition takes five arguments: the name of the theory, a name of 
the definition equation, any conditions on the rule, the left and
right hand sides of the equality.  e.g. the definitional rule,
(\ref{def:plus1} above): $0 + Y = Y$ in
the arithmetic theory\index{arithmetic theory} is written as
\begin{verbatim}
definition arithmetic plus1 trueP (app plus (tuple [zero, Y])) Y.
\end{verbatim}\index{plus1}
The theory is called {\tt arithmetic}, the rule is called {\tt plus1},
there are  
no conditions so this is simply true ({\tt trueP}\index{trueP}) the
LHS is $Y + 0$ and the RHS is $Y$.  You can see this definition by
looking in {\tt arithmetic.mod} in the theories directory of the
\lclam\ distribution.

Lemmas can be started by the keywords {\tt lemma}\index{lemma} or {\tt
  axiom}\index{axiom}.  These take an additional argument to {\tt
  definition}\index{definition} which indicates the direction in which
they may be used.  If the lemma is $X = Y$ then the keyword is {\tt
  equiv}\index{equiv} indicating that it can be used to rewrite
rule\index{rewrite rule} in either direction.

\subsubsection*{Signature files and the pre and post-amble of your
theory file}

Theory files should be called {\tt mytheory.mod} (where mytheory is
the name of your theory).  They should begin with the declaration 

\begin{verbatim}
module mytheory.
\end{verbatim}\index{module}

They should then state the name of a file they wish to accumulate.
{\tt objlists} will give you induction over natural numbers and lists
and so is a good general file to accumulate.  You will also need to
accumulate the file specifying the rule of the logic you intend to
use, in this case this should be {\tt
  constructive\_logic}\index{constructive\_logic}.  It is unwise to  
accumulate more than one file plus the logic file (for technical \lprolog\ reasons see chapter~\ref{developer}) so it
is a good idea to impose a linear structure.  If you have more than
one theory file you wish to use then the first should
accumulate objlists\index{objlists theory} and the second should
accumulate the first.  Accumulation commands should immediately follow 
the module command.  NB. Standard Prolog conventions still hold,
i.e. names beginning with capitals are treated as variables.

\begin{verbatim}
accumulate objlists.
accumulate constructive_logic.
\end{verbatim}\index{accumulate}
You will also need a signature file {\tt mytheory.sig}\index{signature 
file}.  This should start

\begin{verbatim}
sig mytheory.

accum_sig objlists.
accum_sig constructive_logic.

type mytheory theory.
\end{verbatim}
The signature file declares the types of any constants you've
introduced.  So the signature file for arithmetic contains
\begin{verbatim}
type    nat     osyn.

type    zero    osyn.
type    s       osyn.
type    plus    osyn.
type    otimes  osyn.

type    plus1   rewrite_rule.
type    plus2   rewrite_rule.

type assp       query.
\end{verbatim}
As you can see the type of all the syntax constants you've introduced
is {\tt osyn}\index{osyn}.  The type of definitions, lemmas and
axioms\index{definition}\index{lemma}\index{axiom} is {\tt
rewrite\_rule}\index{rewrite\_rule} and the type for goals\index{goal} 
is {\tt query}\index{query}.

Both module and signature files should end with the keyword {\tt
end}\index{end} (this is the only clause/statement that should {\em
not} conclude with a full stop).


\subsubsection*{Create a theory file for the RevQrev conjecture}
To prove the conjecture \lclam\ will need the definitions
of $rev$\index{$rev$} and $qrev$\index{$qrev$} shown in equations
(\ref{def:rev1}), (\ref{def:rev2}), (\ref{def:qrev1}) and
(\ref{def:qrev2}).
As it happens $rev$ is already present in
\lclam\ (called {\tt orev}\index{orev} in the objlists
theory\index{objlists theory}) as is append ({\tt oapp}\index{oapp}) so
only $qrev$ needs to be newly defined in a separated file
(see figure \ref{revqrevcrib} for some of the syntax you will
need for built-in functions and constructors).  The list type is
written {\tt (olist X)}\index{olist}.
\begin{figure}[htb]
\begin{center}
\begin{tabular}{|l|l|} \hline
Mathematical Notation & $\lambda${\clam} Syntax \\ \hline
$nil$\index{$nil$} & {\tt onil}\index{onil} \\ \hline
$H::T$\index{$::$} & {\tt (app ocons (tuple [H, T]))}\index{ocons} \\ \hline
$A<>B$\index{$<>$} & {\tt (app oapp (tuple [A, B]))}\index{oapp} \\ \hline
$rev(A)$\index{$rev$} & {\tt (app orev A)}\index{orev} \\ \hline
\end{tabular}
\end{center}
\caption{$\lambda${\clam} Syntax needed for RevQrev: Quick Crib}
\label{revqrevcrib}
\end{figure}

When you have finished creating the file you will need to compile it.
Check that {\tt TJPATH}\index{TJPATH} points to all the src
subdirectories in the \lclam\ distribution and to the directory in
which you are working.  Type
\begin{verbatim}
tjcc mytheory.lp
\end{verbatim}

We suggest you perform compilation within emacs since the Teyjus
compiler produces a lot of output you may wish to scroll through.  All
compiler errors are flagged with one of the keywords {\tt Error} or
{\tt Warning} so it is probably easiest just to search the buffer for
these (type {\tt C-r} in XEmacs to search backwards).  Watch out for
syntax errors (called parse errors) caused by missing or extra
brackets and commas in the tuple lists.

Type 
\begin{verbatim}
[prompt] $LCLAM_HOME/bin/lclam mytheory
\end{verbatim}
This should start up \lclam\ using your theory.

You can find example files for RevQrev in \S\ref{revqrevtf}. 

\subsubsection*{Plan your conjecture.}
You do this just as you planned {\tt assp.} in the first part of the
tutorial (except with your conjecture name replacing {\tt assp}).
Make sure you have added all your rewrite rules and the definitions
to the wave rule and sym\_eval lists.  Also add the definitions for
{\tt oapp}\index{oapp} and {\tt orev}\index{orev}, {\tt
  idty}\index{idty} and cancellation of cons cells, {\tt cons\_functional}\index{cons\_functional}.  You may find the commands {\tt
  set\_sym\_eval\_list}\index{set\_sym\_eval\_list} etc. useful for this:
\begin{verbatim}
lclam:
set_induction_scheme_list [list_struct].
Done
lclam:
set_sym_eval_list [qrev1, qrev2, orev1, orev2, oapp1, oapp2, idty, cons_functional].
Done
lclam:
set_wave_rule_to_sym_eval.
Done
\end{verbatim}

It should become obvious that there is problem.  Sooner or later you
will get bored of watching the plan go past, give it a while though so
you get a good output to analyse.  You will have to kill it by typing
control-C, or is you are using ProofGeneral, just click on the Stop
button on the PG toolbar.  Scroll back up and try and see if there is
something missing.

The plan is diverging -- it keeps performing induction again and
again.  There are a number of ways you might patch this -- the
simplest is to add a
lemma.  

The planner can't effectively handle the expression 
$$qrev(L2, L3::nil) <> L4$$
which appears in the first step case after weak fertilisation
(rewriting with a hypothesis).\index{fertilisation!weak}

You need the lemma:
$$qrev(A, B) <> C = qrev(A, B <> C)$$
You could just add this as another rule to the list of equations in
your library file (calling it {\tt revqrevlemma} perhaps) but it is
always possible that some ad hoc lemma like this is false so it is
good practice to try and plan it first.

\subsubsection*{Add this as a conjecture to your theory file\index{theory
file} and recompile}
A sample file is shown in
\S\ref{revqrevlemma}.  
Run this new file, set the various lists as above and plan it.

\subsubsection*{Once it is planned correctly add
this lemma to the sym\_eval and wave\_rule lists to plan
{\tt orevqrev}\index{orevqrev} again.}
Once again you can find this library file in \S\ref{revqrevlemma}.

Once it has been successfully planned you can convert the goal to a
lemma for future backward reasoning by typing {\tt qed bwd
  revqrevlemma}.  The new rewrite rule can then be added to one of the
rule lists by typing:
\begin{verbatim}
add_to_sym_eval_list [(user_rewrite ``revqrevlemma_bwd'')].
\end{verbatim}

RevQrev can also be proved with the lemma, the associativity of
append\index{associativity of append} ({\tt
  ass\_oapp}\index{ass\_oapp}), which is one of the rewrite
rules\index{rewrite rule} in the objlists theory\index{objlists
  theory}.  In fact if all the rewrites in that theory were switched
on then the proof would have gone through straight away.

The theorem can also be proved using the built-in critics.  More (but
not much) on
this in \S~\ref{sec:tut:critics}.

\section{Writing Your Own Methods\index{method}}

The last major aspects to using \lclam\ are writing and experimenting
with your own methods and critics.  This tutorial doesn't cover critic
construction since this is rather an advanced topic.  To demonstrate
method construction, consider a simple theorem about the predicate
\emph{divides} defined as:
$$divides \: x \: y = \exists z. (y = (x * z))$$
$divides$ is a function from two natural numbers\index{natural number}
to true or false 
(i.e. a predicate).   The conjecture we will try to plan is:
$$\forall x. (divides \: 0 \: x) \Leftrightarrow (x = 0)$$
To plan this you will need to defined $divides$ (as above) and biimplication 
($\leftrightarrow$): 
\[A \leftrightarrow B \stackrel{\mathrm{def}}{=} (A \rightarrow B)
\wedge (B \rightarrow A)\]
Construct a theory file for this conjecture and try planning this
using the usual {\tt induction\_top normal\_ind}.  You will need to
add your definitions and the rules {\tt plus1, times1} and {\tt idty}
to the symbolic evaluation list before you attempt planning.

When planning \lclam\ will probably loop again, so stop it and have a
look through the output.  Your plan should be going wrong at the goal
$$l2:nat \vdash (\exists l3. (l2 = zero) \rightarrow (l2 = zero))$$
The quantifier is redundant (because $l3$ doesn't appear anywhere
else in the formula) but it is preventing the hypothesis being used to
prove the conclusion.  What we need is some way to discard the
redundant quantifier.

If you look through a standard text book containing rules for manipulating
equations in the predicate calculus\index{predicate calculus} you will
probably find the 
following rule.

$$\frac{\Gamma \vdash \forall x. (P \rightarrow Q)}{\Gamma \vdash
(\exists x. P) \rightarrow Q}\mbox{$x$ does not appear free in $Q$}$$

This inference rule\index{inference rule} could be transformed into a
method\index{method} (so used backwards, basically) and used {\em
  before} {\tt imp\_i}\index{imp\_i} moves the antecedent of the
implication onto the hypothesis list.  So the sequence of proof would
look like.

$$\vdash (\exists V_0. U_1 = 0) \rightarrow U_1 = 0$$
$$\vdash \forall V_0. (U_1 = 0 \rightarrow U_1 = 0) \mbox{ using the
new method}$$
$$\vdash U_1 = 0 \rightarrow U_1 = 0 \mbox{ using {\tt all\_i}}$$
$$U_1 = 0 \vdash U_1 = 0 \mbox{ using {\tt imp\_i}}$$
$$\mbox{ Axiom}$$

We need to write a new method to correspond to this inference rule.

\subsection{The Anatomy of a Method\index{method}}
An atomic method\index{atomic method} in \lclam\ consists of six slots:  Name,
Input\index{input}, Preconditions\index{preconditions},
Effects\index{effects},   
Output\index{output} and Tactic\index{tactic}.  
These appear in library files\index{library file} as seven arguments 
to the predicate {\tt atomic}\index{atomic}.  They are written
as fragments of 
$\lambda$Prolog code.  The method for {\tt all\_i}\index{{\tt all\_i}
  method} appears in the constructive\_logic theory
file\index{constructive logic theory} as:
\begin{verbatim}
atomic constructive_logic all_i 
           (seqGoal (H >>> (app forall (tuple [Otype, (abs A)])))) 
           true
           true 
           (allGoal Otype Y\ (seqGoal (((otype_of Y Otype)::H) >>> (A Y)))) 
           notacticyet.
\end{verbatim}
\begin{enumerate}
\item {\tt constructive\_logic}\index{constructive\_logic} is the theory name.
\item {\tt all\_i} is the method name.  
\item The input\index{input} is a
sequent goal\index{sequent goal} of the form $H \vdash \forall x. A$.
This is written in 
\lclam's internal higher order syntax\index{higher order syntax}.
See \S~\ref{sec:tut:syntax} for details.
\item There are no preconditions\index{precondition}.
\item There are no effects\index{effect}.  
\item {\tt all\_i} removes a universally quantified
  variable\index{quantifiers}, replacing it with an arbitrary constant
  in the object-level\index{object-level}.  In $\lambda${\clam}
  arbitrary constants at the object-level are still universally
  quantified at the meta-level\index{meta-level}.  {\tt
    allGoal}\index{allGoal} is the meta-level universal quantifier.
  Using {\tt allGoal}, the code above introduces a variable {\tt Y}
  which will appear as an arbitrary constant within the object-level
  sequent goal (which is displayed to the user).  It supplies {\tt Y}
  to {\tt A} to replace the universally bound variable.  So the
  resulting goal is the sequent $H \vdash A[x/Y]$
\item {\tt notacticyet}\index{notacticyet} is the
  tactic\index{tactic} that justifies 
the node in the 
plan created by 
this method.  There are no tactics implemented in \lclam\ so {\tt
  notacticyet} is invariably the appropriate place-holder here.
\end{enumerate}

\subsection{Compound Methods\index{compound methods} and the
  Waterfall\index{waterfall}} \lclam\ has a {\em waterfall} of methods
which determines the order in which methods are attempted.  The only
method \lclam\ explicitly calls is the top method supplied by the
user.  This is usually {\tt (induction\_top
  with\_critics)}\index{induction\_top}\index{with\_critics} or {\tt
  (induction\_top normal\_ind)}\index{normal\_ind}.  The first of
these calls \lclam\ with the critics\index{critic} mechanism switched
on.  In this tutorial we have been using {\tt normal\_ind} ( normal
induction) which doesn't use critics for the sake of simplicity.  The
waterfall is determined by the composition of the top method\index{top
  method} and the composition of the methods called by it.  All other
methods are invoked by {\em compound methods} using {\em
  methodicals}\index{methodical} (which glue methods together).  These
are documented in chapter \ref{core}.  Compound methods (those that
use methodicals) are defined using {\tt compound}\index{compound}
which has five arguments, the theory name, the name of the method and
then a method built up from methodicals, the address at which it was
called (usually unused), and lastly preconditions (usually just {\tt
  true}).  For example, the ind\_strat\index{ind\_strat} method is
defined as follows:
\begin{verbatim}
compound induction (ind_strat IndType)
      ( then_meths (induction_meth IndType Scheme Subst)
                   (pair_meth (map_meth (step_case IndType)) (map_meth id_meth)) )
        _
        true.
\end{verbatim}

This applies induction and then applies {\tt id\_meth}\index{id\_meth} 
(do nothing) to the base case(s) and {\tt step\_case}\index{step\_case}
to the step case(s).  Note how it passes the ``sort'' of induction being attempted (e.g. {\tt with\_critics}\index{with\_critics} or {\tt normal\_ind}\index{normal\_ind}) on to the step case method.

Once you have defined a method you need to determine the appropriate
place in the waterfall for it to be applied.  This will mean defining
your own top method and possibly other compound methods.  Most often
you will use the predicate {\tt orelse\_meth}\index{orelse\_meth} to
do this.  {\tt orelse\_meth methodA methodB} tries {\tt methodA} and
if that fails then tries {\tt methodB}.  It is used extensively in
common top methods like {\tt induction\_top\_meth} (below) which
defines the list of methods to be tried and the order they are to be
tried in.

\begin{verbatim}
compound induction (induction_top IndType) (complete_meth
                (repeat_meth
                (orelse_meth trivial
                (orelse_meth allFi
                (orelse_meth taut
                (orelse_meth sym_eval
                (orelse_meth existential
                (orelse_meth (repeat_meth generalise)
                         (ind_strat IndType)
        ))))))))
        _
        true.
\end{verbatim}

It is important to decide where in the waterfall you want your method
to appear (e.g. before or after induction?).  

\subsubsection*{Decide where in the waterfall you need your new method}
Look at the original proof attempt and identify the goal to which you
want your method to apply.  It is important that your new method is
attempted in the waterfall {\em before} the method that is being
called on the goal in the failed attempt.  To see the waterfall you
can look at the method descriptions in chapter \ref{theories}.  Be
aware that the method you are interested in may not be called by {\tt
  top\_meth} but by one of the compound methods in {\tt top\_meth} such as {\tt
  taut}\index{taut} or {\tt ind\_strat}\index{ind\_strat}.

A good rule-of-thumb is to place your new method in the waterfall
directly before the old method that was used in the failed proof
attempt. This analysis should lead you to place your new method before
{\tt imp\_i}\index{imp\_i} in the {\tt taut} method.

\subsubsection*{Define a method for the new inference rule and 
a new {\tt taut} method and top method to call it}

The best way to program new methods is generally by analogy to similar
methods like {\tt all\_i} and {\tt imp\_i}.  (look in the constructive
logic theory file)

Fortunately, you do not need to worry about the side condition on the
inference rule\index{inference rule} ($x$ not free in $Q$) because the
implementation of the syntax in $\lambda${\clam} takes care of this
for you.

\subsubsection*{Try planning the conjecture once more}

To call your top method\index{top method} you will need to use

\begin{verbatim}
lclam:
pds_plan my_top_meth divides_zero.
\end{verbatim}
where {\tt my\_top\_meth} is the name of your top method.

A sample file is shown in \S\ref{dividestaut}.

\section{Critics}
\label{sec:tut:critics}
On the whole we would advise using {\tt induction\_top
  with\_critics}\index{induction\_top}\index{with\_critics} as a top
method not {\tt induction\_top
  normal\_ind}\index{induction\_top}\index{normal\_ind} since this
allows the use of critics to patch rippling proofs, allowing more to
go through.  The critics code is rather complicated and so it is not
really possible to provide a tutorial on its usage here.  To get an
idea about how critics work in \lclam\ it is best to look in the {\tt
  wave}\index{wave theory} theory file and {\tt
  wave\_critics}\index{wave\_critics} theory file for examples and
read the discussion in chapter~\ref{core}.

\section{Automating Command Line interactions}
You are likely to get tired of continually having to type certain sets
of instructions into the command line.  The best way to take care of
this is to use the ProofGeneral interface which allows you to store
proof scripts in a .lcm file.

See also chapter~\ref{mktheory} for a discussion of the {\tt
  benchmark\_plan}\index{benchmark\_plan} predicate which helps with
the automation of benchmarking tasks.

\section{Conclusion}
This tutorial has attempted to overview the main tasks involved in
using and experimenting with the \lclam\ proof
planning system.  It has not discussed in any detail the methods and
methodicals available in $\lambda${\clam}, nor has it provided any
guidance in the use of $\lambda$Prolog -- an understanding of which is
required for all but the most basic attempts at method creation.
Hopefully it has given a rough framework for more detailed
investigation and pointed you in the direction of the examples and
manuals needed for this.

\section{Further Reading}
At some point when using \lclam\ you might well need to get to grips
with \lprolog.  There are not many good manuals available on \lprolog
at time of writing.  The implementation we use is Teyjus and its web
page with documentation can be found at {\tt
  http://teyjus.cs.umn.edu/}.  A web page dedicated to the language
itself, with some course notes etc. can be found at {\tt
  http://www.cse.psu.edu/~dale/lProlog/}.


\section{RevQrev\index{RevQrev} Library File}
\label{revqrevtf}

\subsection{Signature File}
\begin{verbatim}
sig ex1.

accum_sig objlists.
accum_sig constructive_logic.

type ex1 theory.

type qrev osyn.

type qrev1 rewrite_rule.
type qrev2 rewrite_rule.

type orevqrev query.

end
\end{verbatim}

\subsection{Module File}
\begin{verbatim}
module ex1.

accumulate objlists.
accumulate constructive_logic.

has_otype ex1 qrev ((tuple_type [(olist X), (olist X)]) arrow (olist X)).

definition ex1 qrev1 
        trueP
        (app qrev (tuple [onil, L]))
        L.
definition ex1 qrev2
        trueP
        (app qrev (tuple [(app ocons (tuple [H, T])), L]))
        (app qrev (tuple [T, (app ocons (tuple [H, L]))])).

top_goal ex1 orevqrev []
        (app forall (tuple [(olist nat), (abs x\
                (app forall (tuple [(olist nat), (abs y\
                        (app eq (tuple [
        (app oapp (tuple [(app orev x), y])),
        (app qrev (tuple [x, y]))])))])))])).
end
\end{verbatim}

\section{RevQrevLemma Library File}
\label{revqrevlemma}
\subsection{Signature File}
\begin{verbatim}
sig ex2.

accum_sig objlists.
accum_sig constructive_logic.

type ex2 theory.

type qrev osyn.

type qrev1 rewrite_rule.
type qrev2 rewrite_rule.
type oapp_lemma rewrite_rule.

type orevqrev query.
type revqrevlemma query.

end
\end{verbatim}

\subsection{Module File}
\begin{verbatim}
module ex2.

accumulate objlists.
accumulate constructive_logic.

has_otype ex2 qrev ((tuple_type [(olist X), (olist X)]) arrow (olist X)).

definition ex2 qrev1 
        trueP
        (app qrev (tuple [onil, L]))
        L.
definition ex2 qrev2
        trueP
        (app qrev (tuple [(app ocons (tuple [H, T])), L]))
        (app qrev (tuple [T, (app ocons (tuple [H, L]))])).

lemma ex2 oapp_lemma equiv
        trueP
        (app oapp (tuple [(app qrev (tuple [L2, L3])), L4]))
        (app qrev (tuple [L2, (app oapp (tuple [L3, L4]))])).

top_goal ex2 orevqrev []
        (app forall (tuple [(olist nat), (abs x\
                (app forall (tuple [(olist nat), (abs y\
                        (app eq (tuple [
        (app oapp (tuple [(app orev x), y])),
        (app qrev (tuple [x, y]))])))])))])).

top_goal ex2 revqrevlemma []
        (app forall (tuple [(olist nat), (abs x\
                (app forall (tuple [(olist nat), (abs y\
                        (app forall (tuple [(olist nat), (abs z\
        (app eq (tuple [
        (app oapp (tuple [(app qrev (tuple [x, y])), z])),
        (app qrev (tuple [x, (app oapp (tuple [y, z]))]))])))])))])))])
\end{verbatim}


\section{Divides Library File}
\label{dividestaut}

\subsection{Signature File}
\begin{verbatim}
sig ex3.

accum_sig arithmetic.
accum_sig constructive_logic.

type ex3 theory.

type divides osyn.
type iff osyn.

type divides1 rewrite_rule.

type divides_zero query.

type ex3_top_meth meth.
type ex3_taut meth.
type all_imp_left meth.

end
\end{verbatim}

\subsection{Module File}
\begin{verbatim}
module ex3.

accumulate arithmetic.
accumualte constructive_logic.

has_otype ex3 divides ((tuple_type [nat, nat]) arrow nat).
has_otype ex3 iff ((tuple_type [bool, bool]) arrow bool).

definition ex3 iff1 
        trueP
        (app iff (tuple [A, B]))
        (app and (tuple [(app imp (tuple [A, B])),
                         (app imp (tuple [B, A]))])).

definition ex3 divides1
        trueP
        (app divides (tuple [A, B]))
        (app exists (tuple [nat, (abs c\
                (app eq (tuple [B, (app otimes (tuple [A, C]))])))])).

top_goal ex3 divides_zero []
        (app forall (tuple [nat, (abs x\
                (app iff (tuple [
                        (app divides (tuple [zero, x])),
                        (app eq (tuple [x, zero]))])))])).

compound ex3 ex3_top_meth (complete_meth
                (repeat_meth
                (orelse_meth trivial
                (orelse_meth allFi
                (orelse_meth ex3_taut
                (orelse_meth sym_eval
                (orelse_meth existential
                (orelse_meth (repeat_meth generalise)
                         (ind_strat normal_ind)
        ))))))))
        _
        true.

compound ex3 ex3_taut
      (complete_meth
         (repeat_meth
           (orelse_meth trivial
            (orelse_meth false_e
            (orelse_meth all_imp_left
           (orelse_meth neg_i
            (orelse_meth neg_e
            (orelse_meth imp_i
            (orelse_meth all_i
            (orelse_meth and_i
            (orelse_meth and_e
            (orelse_meth or_e
            (orelse_meth imp_e1
            (orelse_meth imp_e2
            (orelse_meth imp_e3
            (orelse_meth imp_e4
            (orelse_meth or_il or_ir)))))))))))))))))
        true.

atomic ex3 all_imp_left
        (seqGoal (H >>> (app imp 
                        (tuple [(app exists (tuple [QType, (abs P)])), Q]))))
        true
        true
        (seqGoal (H >>> (app forall (tuple [QType,
                        (abs X\ (app imp (tuple [(P X), Q])))]))))
        notacticyet.


end
\end{verbatim}
